1, What is predicative pushdown and how it is working in pyspark?
2, What is speculative execution in pyspark?
3, What is partition pruning in pyspark?
4, How to overcome driver OOM(Out Of Memory) in spark?

ğŸ, ğ–ğ¡ğšğ­ ğ¢ğ¬ ğğ«ğğğ¢ğœğšğ­ğ ğğ®ğ¬ğ¡ğğ¨ğ°ğ§ ğšğ§ğ ğ‡ğ¨ğ° ğƒğ¨ğğ¬ ğˆğ­ ğ–ğ¨ğ«ğ¤ ğ¢ğ§ ğğ²ğ’ğ©ğšğ«ğ¤?
Predicate Pushdown is an optimization technique used to enhance query performance by filtering data at the source rather than after it has been loaded into memory. 
ğ‡ğ¨ğ° ğ¢ğ­ ğ–ğ¨ğ«ğ¤ğ¬ ğ¢ğ§ ğğ²ğ’ğ©ğšğ«ğ¤:
ğŸ˜ When a filter operation (e.g., df.filter(...)) is executed, Spark analyzes the query to determine if it can push the filter conditions down to the data source.
ğŸ˜ If the data source supports predicate pushdown (e.g., Parquet, ORC), Spark modifies the query to filter data during the read operation.

ğŸ, ğ–ğ¡ğšğ­ ğ¢ğ¬ ğ¬ğ©ğğœğ®ğ¥ğšğ­ğ¢ğ¯ğ ğğ±ğğœğ®ğ­ğ¢ğ¨ğ§ ğ¢ğ§ ğ©ğ²ğ¬ğ©ğšğ«ğ¤?
Speculative Execution is a feature designed to address the problem of straggler tasksâ€”tasks that lag significantly behind others during execution. It helps ensure that long-running tasks do not slow down the overall job.
ğ‡ğ¨ğ° ğ¢ğ­ ğ–ğ¨ğ«ğ¤ğ¬:
ğŸ˜ Spark monitors task progress. If a task is running slower than its peers, it can launch a duplicate instance of that task on a different node.
ğŸ˜ Both the original and speculative tasks run concurrently, and once one of them finishes, the other is canceled.
ğŸ˜ This parallel execution can reduce overall job completion time.

ğŸ‘. ğ–ğ¡ğšğ­ ğ¢ğ¬ ğğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ ğğ«ğ®ğ§ğ¢ğ§ğ  ğ¢ğ§ ğğ²ğ’ğ©ğšğ«ğ¤?
Partition Pruning is an optimization strategy that minimizes the amount of data read from disk by excluding unnecessary partitions based on filter criteria applied in queries.
ğ‡ğ¨ğ° ğ¢ğ­ ğ–ğ¨ğ«ğ¤ğ¬:
ğŸ˜ When loading partitioned data (e.g., partitioned by date or category) and applying filters referencing the partition columns (e.g., df.filter(df["date"] == '2024-01-01')), Spark can skip scanning partitions that do not meet the filter conditions.
ğŸ˜ This technique is particularly effective for large datasets, leading to reduced I/O and faster query responses.

ğŸ’. ğ‡ğ¨ğ° ğ­ğ¨ ğğ¯ğğ«ğœğ¨ğ¦ğ ğƒğ«ğ¢ğ¯ğğ« ğğ®ğ­ ğğŸ ğŒğğ¦ğ¨ğ«ğ² (ğğğŒ) ğ¢ğ§ ğ’ğ©ğšğ«ğ¤?
Driver Out Of Memory (OOM) errors occur when the Spark driver exceeds its memory allocation,
ğˆğ§ğœğ«ğğšğ¬ğ ğƒğ«ğ¢ğ¯ğğ« ğŒğğ¦ğ¨ğ«ğ²:
ğŸ˜ Adjust the driver memory using the --driver-memory flag (e.g., spark.driver.memory=4g) to allocate more memory.
ğğ©ğ­ğ¢ğ¦ğ¢ğ³ğ ğƒğšğ­ğš ğ’ğğ«ğ¢ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§:
ğŸ˜ Use Kryo serialization for better performance by setting spark.serializer to org.apache.spark.serializer.KryoSerializer.
ğğ«ğ¨ğšğğœğšğ¬ğ­ ğ•ğšğ«ğ¢ğšğ›ğ¥ğğ¬:
ğŸ˜ For large datasets or lookup tables, use broadcast variables to reduce memory consumption
